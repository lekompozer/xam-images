#!/usr/bin/env python3
"""
Crawl all image/video links from an anh.moe album.
Output: prints full-res CDN URLs + collects unique IDs.

Usage:
  python3 crawl_anhmoe_album.py <album_url>

Example:
  python3 crawl_anhmoe_album.py "https://anh.moe/album/G%C3%81I-XINH-4.Ww3iH"
"""

import re, sys, time, json, os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
    ),
    "Referer": "https://anh.moe/",
}
BASE = "https://anh.moe"
DELAY = 1.0

# CDN image patterns — only /b/ bucket is a reliable image CDN.
# /s11/ redirects to amvideos.cfd (video CDN, 404 for images)
# /s4/  redirects to anh-cdn.cyou (unreliable, 404)
CDN_IMG_RE = re.compile(
    r'https://cdn\.save\.moe/b/[^\s"\'<>]+\.(?:jpg|jpeg|png|webp|gif)',
    re.I,
)

session = requests.Session()
session.headers.update(HEADERS)


def get(url, retries=3):
    for i in range(retries):
        try:
            r = session.get(url, timeout=15)
            r.raise_for_status()
            return r
        except Exception as e:
            print(f"  [warn] {e} (attempt {i+1}/{retries})", file=sys.stderr)
            time.sleep(2)
    return None


def full_res(url: str) -> str:
    # strip .md. / .th. / .fr. size suffixes
    return re.sub(r"\.(md|th|fr)\.", ".", url)


def current_page_num(url: str) -> int:
    """Extract ?page=N from URL, default 1."""
    qs = parse_qs(urlparse(url).query)
    try:
        return int(qs.get("page", ["1"])[0])
    except ValueError:
        return 1


def scrape_album_page(url):
    """Scrape one album page, return (full_res_images, next_page_url_or_None)."""
    r = get(url)
    if not r:
        return [], None
    soup = BeautifulSoup(r.text, "html.parser")

    # Collect CDN image links from raw HTML
    SKIP = {"favicon", "logo_", "default_", "system/", "svg/"}
    seen_raw, seen_fr = set(), set()
    results = []
    for u in CDN_IMG_RE.findall(r.text):
        if u in seen_raw:
            continue
        seen_raw.add(u)
        if any(s in u for s in SKIP):
            continue
        fr = full_res(u)
        if fr not in seen_fr:
            seen_fr.add(fr)
            results.append(fr)

    # Find next page: look for page=N+1 with seek= token
    # MUST use N+1 from current URL — cannot just pick first page= link
    n = current_page_num(url)
    next_url = None
    for a in soup.select("a[href]"):
        href = a.get("href", "")
        if f"page={n + 1}" in href and "seek=" in href:
            next_url = urljoin(BASE, href)
            break

    return results, next_url


def crawl_album(start_url):
    all_urls = []
    seen_pages = set()
    url = start_url
    page_num = 1

    while url:
        if url in seen_pages:
            print(f"  [loop detected] stopping.")
            break
        seen_pages.add(url)
        print(f"[page {page_num}] {url}")
        imgs, nxt = scrape_album_page(url)
        print(f"  → {len(imgs)} unique full-res items found")
        all_urls.extend(imgs)
        url = nxt
        page_num += 1
        if nxt:
            time.sleep(DELAY)

    # Final dedup maintaining order
    seen = set()
    deduped = []
    for u in all_urls:
        if u not in seen:
            seen.add(u)
            deduped.append(u)

    return deduped


# ── TAGS-DATA UPDATER ──────────────────────────────────────
def update_tags_data(tag: str, new_ids: list, tags_file="tags-data.js"):
    """
    anh.moe images don't have integer IDs like xamvn — they use CDN URLs.
    We store the URLs directly in a separate key in TAGS_DATA.
    Format: TAGS_DATA.album_items[tag] = ["https://...", ...]
    """
    if not os.path.exists(tags_file):
        print(f"[warn] {tags_file} not found, skipping update.")
        return

    with open(tags_file, encoding="utf-8") as f:
        content = f.read()

    # Parse existing JSON blob
    m = re.search(r"window\.TAGS_DATA\s*=\s*(\{.*\})\s*;", content, re.DOTALL)
    if not m:
        print("[error] Could not parse TAGS_DATA from tags-data.js")
        return

    data = json.loads(m.group(1))

    # Append to existing album_items (deduplicate, preserve order)
    if "album_items" not in data:
        data["album_items"] = {}
    existing = data["album_items"].get(tag, [])
    existing_set = set(existing)
    appended = [u for u in new_ids if u not in existing_set]
    data["album_items"][tag] = existing + appended
    print(
        f"  (existing: {len(existing)}, new: {len(appended)}, total: {len(data['album_items'][tag])})"
    )

    # Rewrite file
    new_json = json.dumps(data, ensure_ascii=False, indent=2)
    new_content = (
        f"// Auto-generated by indexlocal.html\nwindow.TAGS_DATA = {new_json};\n"
    )
    with open(tags_file, "w", encoding="utf-8") as f:
        f.write(new_content)

    print(
        f"\n[tags-data.js] Updated album_items['{tag}'] (+{len(new_ids)} new → {len(data['album_items'][tag])} total) → {tags_file}"
    )


# ── MAIN ──────────────────────────────────────────────────
if __name__ == "__main__":
    album_url = (
        sys.argv[1]
        if len(sys.argv) > 1
        else "https://anh.moe/album/G%C3%81I-XINH-4.Ww3iH"
    )
    tag = sys.argv[2] if len(sys.argv) > 2 else "girl-xinh"

    print("=" * 60)
    print(f"Album: {album_url}")
    print(f"Tag  : {tag}")
    print("=" * 60)

    urls = crawl_album(album_url)

    print(f"\n{'='*60}")
    print(f"TOTAL unique items: {len(urls)}")
    print(f"{'='*60}")

    img_count = sum(
        1 for u in urls if not any(u.endswith(e) for e in [".mp4", ".webm", ".mov"])
    )
    vid_count = len(urls) - img_count
    print(f"  Images : {img_count}")
    print(f"  Videos : {vid_count}")

    print("\nFirst 10:")
    for u in urls[:10]:
        print(" ", u)
    print("...")
    print("Last 5:")
    for u in urls[-5:]:
        print(" ", u)

    # Save result to JSON for inspection
    out_file = "anhmoe_album_urls.json"
    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(
            {"tag": tag, "album": album_url, "urls": urls},
            f,
            ensure_ascii=False,
            indent=2,
        )
    print(f"\nSaved all URLs → {out_file}")

    # Update tags-data.js
    update_tags_data(tag, urls)
