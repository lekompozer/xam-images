#!/usr/bin/env python3
"""
Crawl mp4 links from xamvn.bond/forums/3/
Test mode: first 10 threads only
Output: videos.json
"""

import re, json, time, sys
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup

# ── CONFIG ────────────────────────────────────────────────
BASE = "https://xamvn.bond"
FORUM_URL = "https://xamvn.bond/forums/3/"
MAX_THREADS = 500  # set to None to crawl all
DELAY = 0.8  # seconds between requests
OUTPUT = "videos.json"
OUTPUT_JS = "videos-data.js"  # inline JS for file:// access
# CDN pattern – expand regex if other CDN domains appear
CDN_RE = re.compile(r'https?://[^\s"\'<>]+\.mp4', re.IGNORECASE)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/122.0.0.0 Safari/537.36"
    ),
    "Referer": BASE,
}

session = requests.Session()
session.headers.update(HEADERS)


def get(url, retries=3):
    for i in range(retries):
        try:
            r = session.get(url, timeout=15)
            r.raise_for_status()
            return r
        except Exception as e:
            print(f"  [warn] {e} (attempt {i+1}/{retries})", file=sys.stderr)
            time.sleep(2)
    return None


def save_output(threads, total_videos):
    """Write videos.json + videos-data.js (inline JS for file:// access)."""
    payload = {"threads": threads, "total": total_videos}
    with open(OUTPUT, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    with open(OUTPUT_JS, "w", encoding="utf-8") as f:
        f.write("// Auto-generated by crawl_videos.py — do not edit manually\n")
        f.write("window.VIDEOS_DATA = ")
        json.dump(payload, f, ensure_ascii=False, separators=(",", ":"))
        f.write(";\n")


def get_page(url):
    """Return (soup, raw_html) or (None, '') on failure."""
    r = get(url)
    if not r:
        return None, ""
    return BeautifulSoup(r.text, "html.parser"), r.text


def get_soup(url):
    soup, _ = get_page(url)
    return soup


# ── STEP 1: collect thread URLs from category pages ───────
def get_thread_urls(max_threads=None):
    threads = []
    page = 1
    while True:
        url = FORUM_URL if page == 1 else f"{FORUM_URL}page-{page}"
        print(f"[category] page {page}: {url}")
        soup = get_soup(url)
        if not soup:
            break

        # XenForo thread link pattern
        links = soup.select("a[href*='/threads/']")
        new = []
        seen = set(threads)
        for a in links:
            href = a.get("href", "")
            # keep root thread page only (skip /post- anchors etc)
            m = re.match(r"(/threads/\d+/)$", href)
            if not m:
                m = re.match(r".+(/threads/\d+/)$", href)
            if not m:
                # try full url
                m = re.match(r"https?://[^/]+(/threads/\d+/)(\?.*)?$", href)
                if m:
                    href = BASE + m.group(1)
                else:
                    continue
            else:
                href = urljoin(BASE, href)
            if href not in seen:
                seen.add(href)
                new.append(href)

        if not new:
            print("  → no more threads, stopping.")
            break

        threads.extend(new)
        print(f"  → found {len(new)} threads (total {len(threads)})")

        if max_threads and len(threads) >= max_threads:
            threads = threads[:max_threads]
            break

        # check if there's a next page
        next_btn = soup.select_one(
            "a.pageNav-jump--next, a[rel='next'], .pagination a.next"
        )
        if not next_btn:
            break
        page += 1
        time.sleep(DELAY)

    return threads


# ── STEP 2: extract mp4 links from a single thread (all pages) ──
def get_mp4s_from_thread(thread_url):
    mp4s = []
    seen_mp4 = set()
    page = 1
    title = ""

    while True:
        url = thread_url if page == 1 else f"{thread_url}page-{page}"
        soup, raw_html = get_page(url)
        if not soup:
            break

        # grab thread title once
        if page == 1:
            title = ""
            # 1) og:title is most reliable – format: "Forum - Thread Title | Site Name"
            og = soup.select_one("meta[property='og:title']")
            if og and og.get("content"):
                raw = og["content"].strip()
                # strip site suffix: " | Xamvn - ..." or " | XAMVN"
                if " | " in raw:
                    raw = raw[: raw.rfind(" | ")].strip()
                if raw and raw.lower() not in ("xamvn", "xam vn"):
                    title = raw
            # 2) <title> tag fallback: "Thread Title | Site Name"
            if not title:
                t_tag = soup.select_one("title")
                if t_tag:
                    raw = t_tag.get_text(strip=True)
                    if " | " in raw:
                        raw = raw[: raw.rfind(" | ")].strip()
                    if raw and raw.lower() not in ("xamvn", "xam vn"):
                        title = raw
            # 3) h1.p-title-value last resort (may have prefix text merged in)
            if not title:
                t = soup.select_one("h1.p-title-value")
                if t:
                    candidate = t.get_text(" ", strip=True)
                    if candidate.lower() not in ("xamvn", "xam vn"):
                        title = candidate
            # 4) absolute fallback: thread URL
            if not title:
                title = thread_url

        # 1) raw html scan – catches JS blobs, meta tags, everywhere
        for u in CDN_RE.findall(raw_html):
            if u not in seen_mp4:
                seen_mp4.add(u)
                mp4s.append(u)

        # 2) <video src>, <source src> (explicit tags)
        for tag in soup.select("video[src], source[src]"):
            src = tag.get("src", "")
            if ".mp4" in src.lower() and src not in seen_mp4:
                seen_mp4.add(src)
                mp4s.append(src)

        # 3) any attribute containing .mp4
        for tag in soup.find_all(True):
            for attr in ("href", "data-url", "data-src", "data-original", "content"):
                val = tag.get(attr, "")
                if val and ".mp4" in val.lower():
                    for u in CDN_RE.findall(val):
                        if u not in seen_mp4:
                            seen_mp4.add(u)
                            mp4s.append(u)

        # next thread page?
        next_btn = soup.select_one(
            "a.pageNav-jump--next, a[rel='next'], .pagination a.next"
        )
        print(
            f"    page {page}: {'✓ VIDEO ' + str(len(mp4s)) if mp4s else 'no mp4'}  next={'yes' if next_btn else 'END'}"
        )
        if not next_btn:
            break
        page += 1
        time.sleep(DELAY)

    return {"title": title, "url": thread_url, "videos": mp4s}


def fetch_title_only(thread_url):
    """Fetch page 1 only and return the thread title string."""
    soup, _ = get_page(thread_url)
    if not soup:
        return thread_url
    title = ""
    og = soup.select_one("meta[property='og:title']")
    if og and og.get("content"):
        raw = og["content"].strip()
        if " | " in raw:
            raw = raw[: raw.rfind(" | ")].strip()
        if raw and raw.lower() not in ("xamvn", "xam vn"):
            title = raw
    if not title:
        t_tag = soup.select_one("title")
        if t_tag:
            raw = t_tag.get_text(strip=True)
            if " | " in raw:
                raw = raw[: raw.rfind(" | ")].strip()
            if raw and raw.lower() not in ("xamvn", "xam vn"):
                title = raw
    if not title:
        t = soup.select_one("h1.p-title-value")
        if t:
            candidate = t.get_text(" ", strip=True)
            if candidate.lower() not in ("xamvn", "xam vn"):
                title = candidate
    return title or thread_url


# ── MAIN ──────────────────────────────────────────────────
def main():
    import os, sys

    resume = "--resume" in sys.argv or "-r" in sys.argv
    fix_titles = "--fix-titles" in sys.argv

    print("=" * 60)
    print(f"Crawling forum: {FORUM_URL}")
    print(f"Max threads: {MAX_THREADS}")
    if resume:
        print("Mode: RESUME (load existing + continue)")
    elif fix_titles:
        print("Mode: FIX TITLES (re-fetch bad titles only)")
    print("=" * 60)

    # ── load existing results if resume / fix-titles ──────
    existing_results = []
    existing_urls = set()
    if (resume or fix_titles) and os.path.exists(OUTPUT):
        with open(OUTPUT, encoding="utf-8") as f:
            saved = json.load(f)
        existing_results = saved.get("threads", [])
        existing_urls = {t["url"] for t in existing_results}
        print(f"Loaded {len(existing_results)} existing threads from {OUTPUT}")

    # ── fix-titles mode: re-crawl threads with bad title ──
    if fix_titles:
        bad_titles = {"xamvn", "xam vn", ""}
        to_fix = [
            t
            for t in existing_results
            if t.get("title", "").strip().lower() in bad_titles
            or t.get("title", "").strip() == t.get("url", "")
        ]
        print(
            f"Found {len(to_fix)} threads with bad/missing title — re-fetching (page 1 only)…\n"
        )
        for i, t in enumerate(to_fix, 1):
            print(f"  [{i}/{len(to_fix)}] {t['url']}")
            new_title = fetch_title_only(t["url"])
            t["title"] = new_title
            print(f"  → title: {new_title[:70]}")
            if i % 10 == 0:
                total_videos = sum(len(t.get("videos", [])) for t in existing_results)
                save_output(existing_results, total_videos)
                print(f"  [saved] {i}/{len(to_fix)} titles fixed so far")
            time.sleep(DELAY)
        total_videos = sum(len(t.get("videos", [])) for t in existing_results)
        save_output(existing_results, total_videos)
        print(f"\nFixed {len(to_fix)} titles → saved to {OUTPUT}")
        return

    # ── collect thread URLs ────────────────────────────────
    thread_urls = get_thread_urls(max_threads=MAX_THREADS)
    print(f"\nCollected {len(thread_urls)} threads to scan.\n")

    # in resume mode: skip already-crawled URLs, keep existing results
    if resume:
        new_urls = [u for u in thread_urls if u not in existing_urls]
        print(f"Skipping {len(thread_urls) - len(new_urls)} already-crawled threads.")
        print(f"New threads to crawl: {len(new_urls)}\n")
        thread_urls = new_urls
        results = existing_results
    else:
        results = []

    total_videos = sum(len(t.get("videos", [])) for t in results)

    for i, url in enumerate(thread_urls, 1):
        print(f"\n[{i}/{len(thread_urls)}] {url}")
        data = get_mp4s_from_thread(url)
        results.append(data)
        total_videos += len(data["videos"])
        vid_count = len(data["videos"])
        flag = " ✓ VIDEO" if vid_count else ""
        print(f"  → {vid_count} mp4 link(s){flag}  |  {data['title'][:70]}")

        # incremental save every 10 threads
        if i % 10 == 0:
            save_output(results, total_videos)
            print(f"  [saved] {len(results)} threads so far → {OUTPUT}")

        time.sleep(DELAY)

    # ── summary ──────────────────────────────────────────
    print("\n" + "=" * 60)
    print(f"DONE — {len(results)} threads, {total_videos} total mp4 links")
    threads_with_video = [r for r in results if r["videos"]]
    print(f"Threads WITH videos : {len(threads_with_video)}")
    print(f"Threads WITHOUT     : {len(results) - len(threads_with_video)}")
    if total_videos:
        print(f"\nEstimated IndexedDB size:")
        avg_url_bytes = 80  # ~average cdn url length
        est_bytes = total_videos * avg_url_bytes
        print(f"  ~{total_videos} URLs × {avg_url_bytes}B = {est_bytes/1024:.1f} KB")
        print(f"  (IndexedDB handles 50MB+ per origin easily)")

    # ── write output ──────────────────────────────────────
    save_output(results, total_videos)
    print(f"\nSaved → {OUTPUT} + {OUTPUT_JS}")


if __name__ == "__main__":
    main()
